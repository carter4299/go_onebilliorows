package main

import (
	"bufio"
	"fmt"
	"io"
	"log"
	"os"
	"strconv"
	"strings"
	"sync"

	"golang.org/x/exp/mmap"
)

type Row struct {
	station     string
	measurement float64
}

type Data struct {
	rows []Row
}

type Output struct {
	stations []string
	mean     []float64
	min      []float64
	max      []float64
}

/* EXAMPLE INPUT
Hamburg;12.0
Bulawayo;8.9
Palembang;38.8
St. John's;15.2
Cracow;12.6
Bridgetown;26.9
Istanbul;6.2
Roseau;34.4
Conakry;31.2
Istanbul;23.0
*/

func processLine(line string, lineNumber int) {
	parts := strings.Split(line, ";")
	/*if len(parts) != 2 {
		log.Printf("Error parsing line %d: unexpected format: %s", lineNumber, line)
		return
	}*/

	station := parts[0]
	measurement, err := strconv.ParseFloat(parts[1], 64)

	if err != nil {
		log.Printf("Error parsing measurement on line %d: %s, %s, %f", lineNumber, line, station, measurement)
		return
	}
}

func processChunk(chunk []byte, chunkNumber int, wg *sync.WaitGroup) {
	defer wg.Done()

	scanner := bufio.NewScanner(NewChunkReader(chunk))
	lineNumber := 1
	for scanner.Scan() {
		processLine(scanner.Text(), lineNumber)
		lineNumber++
	}
	/*if err := scanner.Err(); err != nil && err != io.EOF {
		log.Printf("Error scanning chunk %d: %v", chunkNumber, err)
	}*/
}

type ChunkReader struct {
	data []byte
	pos  int
}

func NewChunkReader(data []byte) *ChunkReader {
	return &ChunkReader{data: data}
}

func (r *ChunkReader) Read(p []byte) (int, error) {
	if r.pos >= len(r.data) {
		return 0, io.EOF
	}
	n := copy(p, r.data[r.pos:])
	r.pos += n
	return n, nil
}

func readLargeFile(filePath string, chunkSize int) error {
	reader, _ := mmap.Open(filePath)
	/*if err != nil {
		return fmt.Errorf("failed to open file: %w", err)
	}*/
	defer reader.Close()

	fileInfo, _ := os.Stat(filePath)
	/*if err != nil {
		return fmt.Errorf("failed to get file info: %w", err)
	}*/

	fileSize := fileInfo.Size()
	fmt.Printf("File size: %d bytes\n", fileSize)

	var wg sync.WaitGroup
	for offset := int64(0); offset < fileSize; {
		remainingSize := fileSize - offset
		currentChunkSize := int64(chunkSize)
		if remainingSize < currentChunkSize {
			currentChunkSize = remainingSize
		}

		largeChunk := make([]byte, currentChunkSize+1024)
		n, _ := reader.ReadAt(largeChunk, offset)
		/*if err != nil && err != io.EOF {
			return fmt.Errorf("failed to read chunk: %w", err)
		}*/
		actualChunkSize := int64(n)
		if idx := strings.LastIndexByte(string(largeChunk[:n]), '\n'); idx >= 0 {
			actualChunkSize = int64(idx + 1)
		}

		chunk := largeChunk[:actualChunkSize]
		offset += actualChunkSize

		wg.Add(1)
		go processChunk(chunk, int(offset/int64(chunkSize)), &wg)
	}

	wg.Wait()
	return nil
}

func main() {
	// n_rows := 1000000000
	filePath := "large_dataset.txt"
	// sep := ";"
	chunkSize := 64 * 1024 * 1024 // 64 MB chunks

	if err := readLargeFile(filePath, chunkSize); err != nil {
		log.Fatalf("Error reading file: %v", err)
	}
}
